{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Replication_Assignment_Ioana_Munteanu.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMknYTGOFG9HFuLWW4CMf+5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/munteanuic/RankALS/blob/main/Replication_Assignment_Ioana_Munteanu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4tchrtJT6VH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqzRH8_dUdSs",
        "outputId": "4cf2defc-2532-44d6-cdba-36711e907365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: surprise in /usr/local/lib/python3.7/dist-packages (0.1)\n",
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.7/dist-packages (from surprise) (1.1.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing\n",
        "I decided to use MovieLens 100K movie ratings [1]. MovieLens data sets were collected by the GroupLens Research Project\n",
        "at the University of Minnesota.\n",
        " \n",
        "This data set consists of:\n",
        "  - 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
        "  - Each user has rated at least 20 movies. \n",
        "  - Simple demographic info for the users (age, gender, occupation, zip)"
      ],
      "metadata": {
        "id": "OaTV6urryMC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Dataset\n",
        "data = Dataset.load_builtin('ml-100k')"
      ],
      "metadata": {
        "id": "ixByGPILUdyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Source: CSCI 5123 Fall 2021 Assignment 2'''\n",
        "from surprise.model_selection import KFold\n",
        "import copy\n",
        "tmp = pd.DataFrame(data.raw_ratings, columns=[\"userId\", \"movieId\", \"rating\", \"tstamp\"])\n",
        "rating_df = tmp.pivot(index='userId', columns='movieId', values='rating')\n",
        "\n",
        "def partition (list_in, n):\n",
        "    random.shuffle(list_in)\n",
        "    return [list_in[i::n] for i in range(n)]\n",
        "\n",
        "non_rated_movies = []\n",
        "for userId in rating_df.index:\n",
        "  tmp = rating_df.loc[userId].loc[rating_df.loc[userId].isna()].index\n",
        "  for movieId in tmp:\n",
        "    non_rated_movies.append((userId, movieId, 0))\n",
        "n_splits=5\n",
        "non_rated_testset = partition(non_rated_movies, n_splits)\n",
        "\n",
        "def my_split(data, n_splits=5, random_state=2021):\n",
        "    \"\"\" \n",
        "    The method to generate the splits of trainset, testset_rated (testset with rated items only), testset_withunrated (testset with unrated items, and set their ''true rating'' as 0)\n",
        "    Parameters: \n",
        "    data: suprise dataset, https://surprise.readthedocs.io/en/stable/dataset.html#surprise.dataset.Dataset\n",
        "    n_splits: number of splits\n",
        "    random_state: seed \n",
        "    Returns: \n",
        "    dataset: a list (length = n_splits) of tuples (trainset, testset_rated, testset_withunrated), \n",
        "            and each of the trainset/testset is a list of tuples (userId, movieId, true rating)\n",
        "  \n",
        "    \"\"\"\n",
        "    kf = KFold(n_splits=n_splits, random_state=random_state)\n",
        "\n",
        "    dataset = []\n",
        "    i = 0\n",
        "    tmp = copy.deepcopy(data)\n",
        "    for trainset, testset_rated in kf.split(tmp):\n",
        "      testset_withunrated = copy.deepcopy(testset_rated)\n",
        "      testset_withunrated.extend(non_rated_testset[i][:len(testset_rated)])\n",
        "      dataset.append((trainset, testset_rated, testset_withunrated))\n",
        "      i += 1\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "Ek6rturWUo7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xpd7eKpa1tLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of RankALS\n",
        "I used the pseudocode in the paper \"Alternating least squares for personalized ranking\" [2]. It is described on page 87 as Algorithm 2. The paper proposes \"an effective approach for the direct minimization of a ranking objective function, without sampling.\" It uses an algorithm similar to ImplicitALS, having the same time complexity.\n",
        "\n",
        "The algorithm has tunable paramaters such as:\n",
        "  - E: number of iterations (defaulted to 10)\n",
        "  - rho: initialization range paramter (number between 0 and 1 randomly generated)\n",
        "  - s: True if item importance weights in the ojective funtcion will be used and False otherwise\n",
        "  - F: number of features (defaulted to 20)\n",
        "\n",
        "The algorithm was mathematically derrived by the authors and then implmented using different statistics: p_bar, p_curl, A_bar, A_curl, p1_bar_bar, p2_bar_bar, p3_bar_bar. Every iteration, there is a P step (Q is constant) and a Q step (P is contant). In the P step, the user X feature matrix is updated and in the Q step, the item X feature matrix is updated."
      ],
      "metadata": {
        "id": "5BAtMVsu1ofi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Helper functions"
      ],
      "metadata": {
        "id": "UH0z_vRGxctF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Paramters:\n",
        "  matrix: a matrix\n",
        "  S: a vector conaining row indices\n",
        "Output: \n",
        "  a submatrix of matrix containg all the rows corresponding to the indices from S\n",
        "'''\n",
        "def submatrix_row(matrix, S):\n",
        "    num_rows, num_cols = matrix.shape\n",
        "    return(matrix[np.ix_(S,np.arange(num_cols))])\n",
        "\n",
        "'''\n",
        "Paramters:\n",
        "  matrix: a matrix\n",
        "  S1: a vector conaining row indices\n",
        "  S2: a vector conaining column indices\n",
        "Output: \n",
        "  a submatrix of matrix containg all the rows corresponding to the indices from S1 intersected with the columns corresponding to the indices from S2\n",
        "'''\n",
        "def submatrix_row_col(matrix, S1, S2):\n",
        "    return(matrix[np.ix_(S1,S2)])\n",
        "\n",
        "'''\n",
        "Paramters:\n",
        "  vector: a matrix\n",
        "  S: a vector conaining row indices\n",
        "Output: \n",
        "  a subvector of vector containg all the rows corresponding to the indices from S\n",
        "  OR prints \"Try transposing\" if the vector is nx1 instead of 1xn\n",
        "'''\n",
        "def subvector(vector, S):\n",
        "    if vector.ndim == 1:\n",
        "        return np.take(vector, S)\n",
        "    else:\n",
        "        print(\"Try transposing\")\n",
        "\n",
        "def is_pos(x): \n",
        "  return x > 0\n",
        "\n",
        "def is_max(x):\n",
        "  return x == 5"
      ],
      "metadata": {
        "id": "ke7z0QiqV_oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.numeric import False_\n",
        "from surprise.prediction_algorithms.predictions import Prediction\n",
        "class RankALS():\n",
        "    def __init__(self, trainset, rho, s = False, E=10, F=20):\n",
        "        self.E = E\n",
        "        self.rho = rho\n",
        "        self.F = F\n",
        "        raw_ratings = pd.DataFrame([x for x in trainset.all_ratings()],columns=[\"userId\", \"movieId\", \"ratings\"])\n",
        "        self.ratings = raw_ratings.pivot(index='userId', columns='movieId', values='ratings')\n",
        "        self.ratings.index = self.ratings.index.map(str)# get the list of userId (type str)\n",
        "        self.ratings.columns = self.ratings.columns.map(str)# get the list of movieId (type str)\n",
        "        self.predict_ratings = pd.DataFrame(index=self.ratings.index, columns=self.ratings.columns)\n",
        "        self.R = self.ratings.to_numpy()\n",
        "        if s == False:\n",
        "          self.s = np.ones(self.R.shape[1])\n",
        "        else:\n",
        "          self.s = []\n",
        "          for i in range(self.R.shape[1]):\n",
        "            R_tr = np.transpose(self.R)\n",
        "            all_rating_user = R_tr[i, :]\n",
        "            bool_arr = is_pos(all_rating_user)\n",
        "            self.s.append(np.where(bool_arr)[0].shape[0])\n",
        "          self.s = np.array(self.s)\n",
        "\n",
        "    def fit(self):\n",
        "        num_users, num_items = self.R.shape\n",
        "        l_curl = np.sum(self.s)\n",
        "        P = np.random.uniform(self.rho * (-1), self.rho, (num_users, self.F))\n",
        "        Q = np.random.uniform(self.rho * (-1), self.rho, (num_items, self.F))\n",
        "\n",
        "        for e in range (self.E):\n",
        "            print (\"Iteration {}\".format(e))\n",
        "            #P-step \n",
        "            Q_tr = np.transpose(Q)\n",
        "            q_curl = np.matmul (Q_tr, self.s)\n",
        "            A_curl = np.matmul (np.matmul(Q_tr, np.diag(self.s)), Q)\n",
        "            z = []\n",
        "\n",
        "            for u in range(num_users):\n",
        "                # calcualate r_u\n",
        "                # https://www.kite.com/python/answers/how-to-find-the-index-of-list-elements-that-meet-a-condition-in-python\n",
        "                all_rating_user = self.R[u, :]\n",
        "                bool_arr = is_pos(all_rating_user)\n",
        "                I_u = np.where(bool_arr)[0]\n",
        "                r_u = np.transpose(submatrix_row_col(self.R, np.array([u]), I_u))\n",
        "                \n",
        "                # calcualate l_bar\n",
        "                l_bar = I_u.shape[0] # get num_items from I_U\n",
        "                z.append(l_bar) # calculate z that will be used in Q-step\n",
        "\n",
        "                # calculate r_bar\n",
        "                r_u_tr = np.transpose(r_u)\n",
        "                r_bar = np.matmul(r_u_tr, np.ones(r_u_tr.shape[1]))\n",
        "                \n",
        "                # calculate q_bar\n",
        "                Q_I_u = submatrix_row(Q, I_u)\n",
        "                Q_I_u_tr = np.transpose(Q_I_u)\n",
        "                q_bar = np.matmul(Q_I_u_tr, np.ones(Q_I_u_tr.shape[1]))\n",
        "\n",
        "                # calculate b_bar\n",
        "                b_bar = np.matmul(Q_I_u_tr, r_u)\n",
        "\n",
        "\n",
        "                # calculate A_bar\n",
        "                A_bar = np.matmul(Q_I_u_tr, Q_I_u)\n",
        "\n",
        "                # calculate r_curl\n",
        "                s_I_u = subvector(self.s, I_u)\n",
        "                s_I_u_tr = np.transpose(s_I_u)\n",
        "                r_curl = np.matmul(s_I_u_tr, r_u)\n",
        "\n",
        "                # calculate b_curl\n",
        "                diag = np.diag(s_I_u)\n",
        "                b_curl = np.matmul(np.matmul(Q_I_u_tr, diag), r_u)\n",
        "                t1 = l_curl * A_bar\n",
        "                t2 = np.matmul(q_bar, np.transpose(q_curl))\n",
        "                t3 = np.matmul(q_curl, np.transpose(q_bar)) \n",
        "                t4 = l_bar * A_curl\n",
        "                M = t1 - t2 - t3 + t4\n",
        "                # print(\"T1: {}, T2: {}, T3: {}, T4: {}\".format(t1,t2,t3,t4))\n",
        "\n",
        "                #calculate y\n",
        "                t1 = np.transpose(b_bar * l_curl)\n",
        "                t2 = (q_bar * r_curl)\n",
        "                t3 = (r_bar * q_curl)\n",
        "                t4 = np.transpose(l_bar * b_curl)\n",
        "                y = t1 - t2 - t3 + t4\n",
        "                y = np.transpose(y)\n",
        "\n",
        "                # update P[u]\n",
        "                P_u = np.matmul(np.linalg.inv(M), y)\n",
        "                P[u] = np.transpose(P_u)\n",
        "\n",
        "            #Q-step\n",
        "            # calculate q_curl\n",
        "            q_curl = np.matmul (Q_tr, self.s)\n",
        "\n",
        "            # calculate A_bar_bar\n",
        "            P_tr = np.transpose(P)\n",
        "            z = np.array(z)\n",
        "            A_bar_bar = np.matmul(np.matmul(P_tr, np.diag(z)), P)\n",
        "            \n",
        "            # calculate p_bar, never used\n",
        "            p1_bar = np.matmul(P_tr, z)\n",
        "            \n",
        "            r_curl, r_bar, Q_bar = np.zeros(num_users), np.zeros(num_users), np.zeros((num_users, self.F))\n",
        "            for u in range(num_users):\n",
        "                # calcualate r_u\n",
        "                all_rating_user = self.R[u, :]\n",
        "                bool_arr = is_pos(all_rating_user)\n",
        "                I_u = np.where(bool_arr)[0]\n",
        "                r_u = np.transpose(submatrix_row_col(self.R, np.array([u]), I_u))\n",
        "\n",
        "                # calculate r_u_curl\n",
        "                r_u_tr = np.transpose(r_u)\n",
        "                s_I_u = subvector(self.s, I_u)\n",
        "                r_curl[u] = (np.matmul(r_u_tr, s_I_u))\n",
        "\n",
        "                # calculate r_bar\n",
        "                r_bar[u] = np.matmul(r_u_tr, np.ones(r_u_tr.shape[1]))\n",
        "\n",
        "                # calculate Q_bar_u\n",
        "                Q_I_u_tr = np.transpose(submatrix_row(Q, I_u))\n",
        "                \n",
        "                # calculate Q_bar\n",
        "                Q_bar[u] = np.matmul(Q_I_u_tr, np.ones(Q_I_u_tr.shape[1]))\n",
        "            \n",
        "            # calculate p2_bar_bar\n",
        "            P_tr = np.transpose(P)\n",
        "            tr = np.trace(np.matmul(P_tr, Q_bar))\n",
        "            p2_bar_bar = P_tr * tr\n",
        "            p3_bar_bar = np.matmul(np.transpose(P), r_bar)\n",
        "\n",
        "            for i in range(num_items):\n",
        "                # calculate r_i\n",
        "                R_tr = np.transpose(self.R)\n",
        "                all_rating_user = R_tr[i, :]\n",
        "                bool_arr = is_pos(all_rating_user)\n",
        "                U_i = np.where(bool_arr)[0]\n",
        "                r_i = np.transpose(submatrix_row_col(R_tr, np.array([i]), U_i))\n",
        "            \n",
        "                # calculate b_bar\n",
        "                P_tr_U_i = np.transpose(submatrix_row(P, U_i))\n",
        "                b_bar = np.matmul(P_tr_U_i, r_i)\n",
        "                \n",
        "                # calculate A_bar\n",
        "                A_bar = np.matmul(P_tr_U_i, submatrix_row(P, U_i))\n",
        "\n",
        "                # calculate A_bar_bar\n",
        "                b_bar_bar = np.matmul(np.matmul(P_tr_U_i, np.diag(subvector(z, U_i))), r_i)\n",
        "\n",
        "                # calculate p1_bar_bar\n",
        "                p1_bar_bar = np.matmul(P_tr_U_i, subvector(r_curl, U_i))\n",
        "\n",
        "                # calculate M\n",
        "                M = l_curl * A_bar + self.s[i] * A_bar_bar\n",
        "\n",
        "                # calculate y\n",
        "                t1 = np.matmul(A_bar, q_curl)\n",
        "                t2 = (np.transpose(b_bar * l_curl))[0]\n",
        "                t3 = p1_bar_bar \n",
        "                t4 = p3_bar_bar*self.s[i]\n",
        "                t5 = np.transpose(b_bar_bar*self.s[i])[0]\n",
        "                y = t1 + t2 - t3 - t4 + t5\n",
        "                q_i = np.matmul(np.linalg.inv(M), y)\n",
        "                if q_i[0] == np.nan:\n",
        "                  Q[i] = np.random.uniform(self.rho * (-1), self.rho, (self.F))\n",
        "                else:\n",
        "                  Q[i] = q_i\n",
        "\n",
        "        pred = np.matmul(P, np.transpose(Q))\n",
        "        pred[pred > 0] = 1\n",
        "        pred[pred <= 0] = 0\n",
        "        self.predict_ratings = pd.DataFrame(data = pred, index = self.ratings.index, columns = self.ratings.columns)\n",
        "        \n",
        "    def test(self, testset):\n",
        "      estimations = []\n",
        "      for userId, movieId, true_rating in testset:\n",
        "        if userId in self.predict_ratings.index and movieId in self.predict_ratings.columns and not np.isnan(self.predict_ratings.loc[userId, movieId]):\n",
        "          pred_rating = self.predict_ratings.loc[userId, movieId]\n",
        "          estimations.append(Prediction(uid=userId, iid=movieId, r_ui=true_rating, est=pred_rating,details={'was_impossible': False}))\n",
        "      return estimations"
      ],
      "metadata": {
        "id": "XOb5uuLsUvEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics and comparison to SVDpp\n",
        "I calculated the error rate and the recall, just like in the paper. Additioanlly, I added three more metrics: precision, RMSE, and MAE. I tested the algorithm both with and without an importance weight. Addionally, I compared the two versions of the RankALS algorithm with SVDpp since they are both algorithms that use matrix factorization and implicit bias. Considering that the Movielens dataset is sparse, the implicit bias is expected to produce better results. "
      ],
      "metadata": {
        "id": "BBGhfftZ9pux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Source: https://surprise.readthedocs.io/en/stable/FAQ.html#how-to-compute-precision-k-and-recall-k\n",
        "Used to calculate precision and recall\n",
        "'''\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
        "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
        "\n",
        "    # First map the predictions to each user.\n",
        "    user_est_true = defaultdict(list)\n",
        "    for uid, _, true_r, est, _ in predictions:\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "\n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Number of relevant items\n",
        "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
        "\n",
        "        # Number of recommended items in top k\n",
        "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
        "\n",
        "        # Number of relevant and recommended items in top k\n",
        "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
        "                              for (est, true_r) in user_ratings[:k])\n",
        "\n",
        "        # Precision@K: Proportion of recommended items that are relevant\n",
        "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
        "\n",
        "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
        "\n",
        "        # Recall@K: Proportion of relevant items that are recommended\n",
        "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
        "\n",
        "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
        "\n",
        "    return precisions, recalls"
      ],
      "metadata": {
        "id": "Xe8Ok0FJ8kkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import accuracy, SVDpp\n",
        "from surprise.model_selection import cross_validate\n",
        "\n",
        "metric_report = pd.DataFrame(index=['RankALS without weight', 'RankALS with weight', 'SVD++'], columns=['error rate', 'precision', 'recall', 'rmse','mae'])\n",
        "split_data = my_split(data)\n",
        "\n",
        "errors0, precs0, recs0, rmses0, maes0 = [], [], [], [], []\n",
        "errors1, precs1, recs1, rmses1, maes1 = [], [], [], [], []\n",
        "errors2, precs2, recs2, rmses2, maes2 = [], [], [], [], []\n",
        "\n",
        "for fold in split_data:\n",
        "  trainset, testset_rated, testset_withunrated = fold\n",
        "  testset_rated = [[x[0], x[1], 1] if x[2] == 5 else [x[0], x[1], 0] for x in testset_rated]\n",
        "  rho = np.random.normal(0,1,1)\n",
        "\n",
        "  print(\"-------Start testing Rank ALS without wighting------\")\n",
        "  als = RankALS(trainset, rho, E = 10) \n",
        "  als.fit()\n",
        "  pred = als.test(testset_rated)\n",
        "\n",
        "  # calculate error rate \n",
        "  right, wrong = 0, 0\n",
        "  for p in pred:\n",
        "    if p.est == p.r_ui:\n",
        "      right += 1\n",
        "    else:\n",
        "      wrong += 1\n",
        "  errors0.append(wrong/(right+wrong))\n",
        "\n",
        "  # calculate precisions and recalls \n",
        "  precisions, recalls = precision_recall_at_k(pred, k=5, threshold=1) # set threshold to 1 since 1 means recommended\n",
        "  precs0.append(sum(prec for prec in precisions.values()) / len(precisions))\n",
        "  recs0.append(sum(rec for rec in recalls.values()) / len(recalls))\n",
        "\n",
        "  # calculate rmses\n",
        "  rmses0.append(accuracy.rmse(pred))\n",
        "\n",
        "  # calculate maes\n",
        "  maes0.append(accuracy.mae(pred))\n",
        "  print(\"-------End testing Rank ALS without weighting------\")\n",
        "\n",
        "  print(\"-------Start testing Rank ALS with weighting------\")\n",
        "  alsW = RankALS(trainset, rho, s = True, E = 10) \n",
        "  alsW.fit()\n",
        "  pred = alsW.test(testset_rated)\n",
        "\n",
        "  # calculate error rate \n",
        "  right, wrong = 0, 0\n",
        "  for p in pred:\n",
        "    if p.est == p.r_ui:\n",
        "      right += 1\n",
        "    else:\n",
        "      wrong += 1\n",
        "  errors1.append(wrong/(right+wrong))\n",
        "\n",
        "  # calculate precisions and recalls \n",
        "  precisions, recalls = precision_recall_at_k(pred, k=5, threshold=1) # set threshold to 1 since 1 means positive rating\n",
        "  precs1.append(sum(prec for prec in precisions.values()) / len(precisions))\n",
        "  recs1.append(sum(rec for rec in recalls.values()) / len(recalls))\n",
        "\n",
        "  # calculate rmses\n",
        "  rmses1.append(accuracy.rmse(pred))\n",
        "\n",
        "  # calculate maes\n",
        "  maes1.append(accuracy.mae(pred))\n",
        "  print(\"-------End testing Rank ALS with weighting------\")\n",
        "\n",
        "  print(\"-------Start testing SVDpp------\")\n",
        "  trainset, testset_rated, testset_withunrated = fold\n",
        "  svdpp = SVDpp(n_factors=20, n_epochs=10)\n",
        "  svdpp.fit(trainset)\n",
        "  pred = svdpp.test(testset_rated)\n",
        "\n",
        "  # calculate error rate \n",
        "  right, wrong = 0, 0\n",
        "  for p in pred:\n",
        "    if p.est == p.r_ui:\n",
        "      right += 1\n",
        "    else:\n",
        "      wrong += 1\n",
        "  errors2.append(wrong/(right+wrong))\n",
        "\n",
        "  # calculate precisions and recalls \n",
        "  precisions, recalls = precision_recall_at_k(pred, k=5, threshold=5) # set threshold to 5 since 5 means psoitive rating\n",
        "  precs2.append(sum(prec for prec in precisions.values()) / len(precisions))\n",
        "  recs2.append(sum(rec for rec in recalls.values()) / len(recalls))\n",
        "\n",
        "  # calculate rmses\n",
        "  rmses2.append(accuracy.rmse(pred))\n",
        "\n",
        "  # calculate maes\n",
        "  maes2.append(accuracy.mae(pred))\n",
        "  \n",
        "  print(\"-------End testing SVDpp------\")\n",
        "metric_report['error rate'] = [sum(errors0)/5, sum(errors1)/5, sum(errors2)/5]\n",
        "metric_report['precision'] = [sum(precs0)/5, sum(precs1)/5, sum(precs2)/5]\n",
        "metric_report['recall'] = [sum(recs0)/5, sum(recs1)/5, sum(recs2)/5]\n",
        "metric_report['rmse'] = [sum(rmses0)/5, sum(rmses1)/5, sum(rmses2)/5]\n",
        "metric_report['mae'] = [sum(maes0)/5, sum(maes1)/5, sum(maes2)/5]\n",
        "\n",
        "print(metric_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV6UFrC4Wcc8",
        "outputId": "b694f29f-26ba-42bd-f3eb-0b0fae9481bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------Start testing Rank ALS without wighting------\n",
            "Iteration 0\n",
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Iteration 6\n",
            "Iteration 7\n",
            "Iteration 8\n",
            "Iteration 9\n",
            "RMSE: 0.5385\n",
            "MAE:  0.2900\n",
            "-------End testing Rank ALS without weighting------\n",
            "-------Start testing Rank ALS with weighting------\n",
            "Iteration 0\n",
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Iteration 6\n",
            "Iteration 7\n",
            "Iteration 8\n",
            "Iteration 9\n",
            "RMSE: 0.5290\n",
            "MAE:  0.2799\n",
            "-------End testing Rank ALS with weighting------\n",
            "-------Start testing SVDpp------\n",
            "RMSE: 0.9155\n",
            "MAE:  0.7271\n",
            "-------End testing SVDpp------\n",
            "-------Start testing Rank ALS without wighting------\n",
            "Iteration 0\n",
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Iteration 6\n",
            "Iteration 7\n",
            "Iteration 8\n",
            "Iteration 9\n",
            "RMSE: 0.5448\n",
            "MAE:  0.2968\n",
            "-------End testing Rank ALS without weighting------\n",
            "-------Start testing Rank ALS with weighting------\n",
            "Iteration 0\n",
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Iteration 6\n",
            "Iteration 7\n",
            "Iteration 8\n",
            "Iteration 9\n",
            "RMSE: 0.5346\n",
            "MAE:  0.2858\n",
            "-------End testing Rank ALS with weighting------\n",
            "-------Start testing SVDpp------\n",
            "RMSE: 0.9344\n",
            "MAE:  0.7385\n",
            "-------End testing SVDpp------\n",
            "-------Start testing Rank ALS without wighting------\n",
            "Iteration 0\n",
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Iteration 6\n",
            "Iteration 7\n",
            "Iteration 8\n",
            "Iteration 9\n",
            "RMSE: 0.5341\n",
            "MAE:  0.2852\n",
            "-------End testing Rank ALS without weighting------\n",
            "-------Start testing Rank ALS with weighting------\n",
            "Iteration 0\n",
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Iteration 6\n",
            "Iteration 7\n",
            "Iteration 8\n",
            "Iteration 9\n",
            "RMSE: 0.5293\n",
            "MAE:  0.2802\n",
            "-------End testing Rank ALS with weighting------\n",
            "-------Start testing SVDpp------\n",
            "RMSE: 0.9299\n",
            "MAE:  0.7343\n",
            "-------End testing SVDpp------\n",
            "-------Start testing Rank ALS without wighting------\n",
            "Iteration 0\n",
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Iteration 6\n",
            "Iteration 7\n",
            "Iteration 8\n",
            "Iteration 9\n",
            "RMSE: 0.5391\n",
            "MAE:  0.2907\n",
            "-------End testing Rank ALS without weighting------\n",
            "-------Start testing Rank ALS with weighting------\n",
            "Iteration 0\n",
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Iteration 6\n",
            "Iteration 7\n",
            "Iteration 8\n",
            "Iteration 9\n",
            "RMSE: 0.5307\n",
            "MAE:  0.2817\n",
            "-------End testing Rank ALS with weighting------\n",
            "-------Start testing SVDpp------\n",
            "RMSE: 0.9259\n",
            "MAE:  0.7309\n",
            "-------End testing SVDpp------\n",
            "-------Start testing Rank ALS without wighting------\n",
            "Iteration 0\n",
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Iteration 6\n",
            "Iteration 7\n",
            "Iteration 8\n",
            "Iteration 9\n",
            "RMSE: 0.5322\n",
            "MAE:  0.2832\n",
            "-------End testing Rank ALS without weighting------\n",
            "-------Start testing Rank ALS with weighting------\n",
            "Iteration 0\n",
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Iteration 6\n",
            "Iteration 7\n",
            "Iteration 8\n",
            "Iteration 9\n",
            "RMSE: 0.5275\n",
            "MAE:  0.2783\n",
            "-------End testing Rank ALS with weighting------\n",
            "-------Start testing SVDpp------\n",
            "RMSE: 0.9346\n",
            "MAE:  0.7362\n",
            "-------End testing SVDpp------\n",
            "                        error rate  precision    recall      rmse       mae\n",
            "RankALS without weight    0.289180   0.180301  0.113516  0.537736  0.289180\n",
            "RankALS with weight       0.281173   0.171534  0.098765  0.530252  0.281173\n",
            "SVD++                     0.998060   0.019954  0.003261  0.928074  0.733409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "RankALS with weight performs slightly better than RankALS without weight in terms of eror rate, rmse and mae, while RankALS without weight slightly outperforms RankALS for recall and precision. The major differences are between both RankALS adn SVD++. The error rate forr SVD++ is 3 times higher than the avergae error rate in rankALS. The precision of SVD++ is 9 times lower than the one of RankaLS. Aditionally, SVD has high values of rmse and mae in comarison to the other two algorithms."
      ],
      "metadata": {
        "id": "8Aep9uPy-6u3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "RankALS outperforms SVD++. RankALS is better in terms of error rate, rmse and mae if significance weights are used, which supports the findings of the paper, but is worse in terms of recall and precision, which disproves the paper."
      ],
      "metadata": {
        "id": "Ef3VGUzJA9AT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sources\n",
        "\n",
        "\n",
        "1.   F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets:\n",
        "History and Context. ACM Transactions on Interactive Intelligent\n",
        "Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages.\n",
        "DOI=http://dx.doi.org/10.1145/2827872\n",
        "2.   Gábor Takács and Domonkos Tikk. 2012. Alternating least squares for personalized ranking. In Proceedings of the sixth ACM conference on Recommender systems (RecSys '12). Association for Computing Machinery, New York, NY, USA, 83–90. DOI:https://doi.org/10.1145/2365952.2365972"
      ],
      "metadata": {
        "id": "sG0Cj_TY9fJ3"
      }
    }
  ]
}